{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import tree,ensemble,cluster\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,mean_squared_error,roc_auc_score,recall_score,precision_score\n",
    "from datetime import timedelta\n",
    "import datetime\n",
    "import pytz\n",
    "from sklearn import model_selection\n",
    "import pickle\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('taxi-train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv('taxi-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1071910, 14), (10828, 11))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape,test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdevasish\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:7123: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort,\n"
     ]
    }
   ],
   "source": [
    "train['flag']=0\n",
    "test['flag']=1\n",
    "test['fare_amount']=1\n",
    "test['tip_amount']=1\n",
    "test['tip_paid']=20\n",
    "data=train.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1082738, 15)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>pickup_longitude</td>\n",
       "      <td>10828.0</td>\n",
       "      <td>-73.977155</td>\n",
       "      <td>0.014385</td>\n",
       "      <td>-73.999992</td>\n",
       "      <td>-73.988146</td>\n",
       "      <td>-73.979932</td>\n",
       "      <td>-73.968395</td>\n",
       "      <td>-73.901093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>pickup_latitude</td>\n",
       "      <td>10828.0</td>\n",
       "      <td>40.756904</td>\n",
       "      <td>0.018265</td>\n",
       "      <td>40.700585</td>\n",
       "      <td>40.744828</td>\n",
       "      <td>40.757743</td>\n",
       "      <td>40.769749</td>\n",
       "      <td>40.799987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>dropoff_longitude</td>\n",
       "      <td>10828.0</td>\n",
       "      <td>-73.975478</td>\n",
       "      <td>0.015812</td>\n",
       "      <td>-73.999988</td>\n",
       "      <td>-73.987152</td>\n",
       "      <td>-73.978599</td>\n",
       "      <td>-73.966107</td>\n",
       "      <td>-73.900187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>dropoff_latitude</td>\n",
       "      <td>10828.0</td>\n",
       "      <td>40.757267</td>\n",
       "      <td>0.018781</td>\n",
       "      <td>40.700232</td>\n",
       "      <td>40.745697</td>\n",
       "      <td>40.758194</td>\n",
       "      <td>40.770290</td>\n",
       "      <td>40.799989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>rate_code</td>\n",
       "      <td>9040.0</td>\n",
       "      <td>1.006416</td>\n",
       "      <td>0.137795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>passenger_count</td>\n",
       "      <td>10828.0</td>\n",
       "      <td>1.706779</td>\n",
       "      <td>1.326389</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>trip_distance</td>\n",
       "      <td>10828.0</td>\n",
       "      <td>1.816876</td>\n",
       "      <td>1.256048</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>12.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>flag</td>\n",
       "      <td>10828.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>fare_amount</td>\n",
       "      <td>10828.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>tip_amount</td>\n",
       "      <td>10828.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>tip_paid</td>\n",
       "      <td>10828.0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     count       mean       std        min        25%  \\\n",
       "pickup_longitude   10828.0 -73.977155  0.014385 -73.999992 -73.988146   \n",
       "pickup_latitude    10828.0  40.756904  0.018265  40.700585  40.744828   \n",
       "dropoff_longitude  10828.0 -73.975478  0.015812 -73.999988 -73.987152   \n",
       "dropoff_latitude   10828.0  40.757267  0.018781  40.700232  40.745697   \n",
       "rate_code           9040.0   1.006416  0.137795   0.000000   1.000000   \n",
       "passenger_count    10828.0   1.706779  1.326389   1.000000   1.000000   \n",
       "trip_distance      10828.0   1.816876  1.256048   0.040000   0.940000   \n",
       "flag               10828.0   1.000000  0.000000   1.000000   1.000000   \n",
       "fare_amount        10828.0   1.000000  0.000000   1.000000   1.000000   \n",
       "tip_amount         10828.0   1.000000  0.000000   1.000000   1.000000   \n",
       "tip_paid           10828.0  20.000000  0.000000  20.000000  20.000000   \n",
       "\n",
       "                         50%        75%        max  \n",
       "pickup_longitude  -73.979932 -73.968395 -73.901093  \n",
       "pickup_latitude    40.757743  40.769749  40.799987  \n",
       "dropoff_longitude -73.978599 -73.966107 -73.900187  \n",
       "dropoff_latitude   40.758194  40.770290  40.799989  \n",
       "rate_code           1.000000   1.000000   5.000000  \n",
       "passenger_count     1.000000   2.000000   6.000000  \n",
       "trip_distance       1.500000   2.300000  12.670000  \n",
       "flag                1.000000   1.000000   1.000000  \n",
       "fare_amount         1.000000   1.000000   1.000000  \n",
       "tip_amount          1.000000   1.000000   1.000000  \n",
       "tip_paid           20.000000  20.000000  20.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1071910 entries, 0 to 1071909\n",
      "Data columns (total 15 columns):\n",
      "vendor_id            1071910 non-null object\n",
      "pickup_datetime      1071910 non-null object\n",
      "dropoff_datetime     1071910 non-null object\n",
      "pickup_longitude     1071910 non-null float64\n",
      "pickup_latitude      1071910 non-null float64\n",
      "dropoff_longitude    1071910 non-null float64\n",
      "dropoff_latitude     1071910 non-null float64\n",
      "rate_code            898694 non-null float64\n",
      "passenger_count      1071910 non-null int64\n",
      "trip_distance        1071910 non-null float64\n",
      "payment_type         1071910 non-null object\n",
      "fare_amount          1071910 non-null float64\n",
      "tip_amount           1071910 non-null float64\n",
      "tip_paid             1071910 non-null int64\n",
      "flag                 1071910 non-null int64\n",
      "dtypes: float64(8), int64(3), object(4)\n",
      "memory usage: 122.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10828 entries, 0 to 10827\n",
      "Data columns (total 15 columns):\n",
      "vendor_id            10828 non-null object\n",
      "pickup_datetime      10828 non-null object\n",
      "dropoff_datetime     10828 non-null object\n",
      "pickup_longitude     10828 non-null float64\n",
      "pickup_latitude      10828 non-null float64\n",
      "dropoff_longitude    10828 non-null float64\n",
      "dropoff_latitude     10828 non-null float64\n",
      "rate_code            9040 non-null float64\n",
      "passenger_count      10828 non-null int64\n",
      "trip_distance        10828 non-null float64\n",
      "payment_type         10828 non-null object\n",
      "flag                 10828 non-null int64\n",
      "fare_amount          10828 non-null int64\n",
      "tip_amount           10828 non-null int64\n",
      "tip_paid             10828 non-null int64\n",
      "dtypes: float64(6), int64(5), object(4)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CSH' 'Cre' 'CRD' 'CAS' 'Cas' 'CRE' 'NOC' 'DIS' 'UNK' 'No ' 'Dis' 'NA ']\n",
      "Unique payment methods\n",
      " ['CSH' 'CRD' 'NOC' 'DIS' 'NA ' 'UNK']\n",
      "Distribution of payment methods\n",
      " CSH    611361\n",
      "CRD    468702\n",
      "NOC      1149\n",
      "NA       1066\n",
      "DIS       329\n",
      "UNK       131\n",
      "Name: payment_type, dtype: int64\n",
      "1.0     905915\n",
      "4.0       1169\n",
      "2.0        385\n",
      "0.0        119\n",
      "5.0         88\n",
      "3.0         52\n",
      "6.0          5\n",
      "99.0         1\n",
      "Name: rate_code, dtype: int64\n",
      "[  1. 100.   4.   2.   3.   0.   5.   6.  99.]\n",
      "No.of records with distance less than zero after cleaning 0\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing for dates\n",
    "# Derived duration, week day and new time as per time zone\n",
    "data.pickup_datetime=pd.to_datetime(data.pickup_datetime)\n",
    "data.dropoff_datetime=pd.to_datetime(data.dropoff_datetime)\n",
    "data['pickup_datezone']=data['pickup_datetime'].apply(lambda x:x.astimezone('US/Eastern'))\n",
    "data['dropoff_datezone']=data['dropoff_datetime'].apply(lambda x:x.astimezone('US/Eastern'))\n",
    "data['duration']=data.dropoff_datetime-data.pickup_datetime\n",
    "data['duration']=data['duration'].apply(lambda x: x.total_seconds())\n",
    "\n",
    "# Cleaning the records with duration less than zero\n",
    "#data=data[data['duration'] >0]\n",
    "data['weekday']=data.pickup_datezone.dt.weekday\n",
    "data['year']=data['pickup_datezone'].apply(lambda x:x.year)\n",
    "data['hours']=data['pickup_datezone'].apply(lambda x:x.time().hour)\n",
    "data['minutes']=data['pickup_datezone'].apply(lambda x:x.time().minute)\n",
    "data['seconds']=data['pickup_datezone'].apply(lambda x:x.time().second)\n",
    "year=pd.get_dummies(data['year'],prefix='year=')\n",
    "data['sec_norm']=((data['hours']*60*60)+(data['minutes']*60)+data['seconds'])/86400\n",
    "data=data.drop(['hours','minutes','seconds','pickup_datetime','dropoff_datetime','year',\\\n",
    "                  'pickup_datezone','dropoff_datezone'],axis=1)\n",
    "#days=pd.get_dummies(data['weekday'],prefix='Day')\n",
    "#hours=pd.get_dummies(data['hour'],prefix='Hour')\n",
    "\n",
    "\n",
    "# Preprocessing for payment_type Recoding and Encoding\n",
    "print(data.payment_type.unique())\n",
    "data['payment_type'].replace('Cre','CRD',inplace=True)\n",
    "data['payment_type'].replace('Cas','CSH',inplace=True)\n",
    "data['payment_type'].replace('Dis','DIS',inplace=True)\n",
    "data['payment_type'].replace('CRE','CRD',inplace=True)\n",
    "data['payment_type'].replace('CAS','CSH',inplace=True)\n",
    "data['payment_type'].replace('Dis','DIS',inplace=True)\n",
    "data['payment_type'].replace('UNK','NA ',inplace=True)\n",
    "data['payment_type'].replace('No ','UNK',inplace=True)\n",
    "print('Unique payment methods\\n',data.payment_type.unique())\n",
    "print('Distribution of payment methods\\n',data.payment_type.value_counts())\n",
    "pay=pd.get_dummies(data['payment_type'], prefix='Pay')\n",
    "\n",
    "#Ordinal encoding of passenger count\n",
    "data['passenger_count_1'] = 1 * (data['passenger_count'] >= 1)\n",
    "data['passenger_count_2'] = 1 * (data['passenger_count'] >= 2)\n",
    "data['passenger_count_3'] = 1 * (data['passenger_count'] >= 3)\n",
    "data['passenger_count_4'] = 1 * (data['passenger_count'] >= 4)\n",
    "data['passenger_count_5'] = 1 * (data['passenger_count'] >= 5)\n",
    "\n",
    "# Pre processing for rate_code\n",
    "print(data.rate_code.value_counts())\n",
    "data.rate_code=data.rate_code.fillna(100)\n",
    "print(data.rate_code.unique())\n",
    "rate=pd.get_dummies(data['rate_code'],prefix='rate')\n",
    "\n",
    "# Concatenation for vendor ID, weekday and payment_type, passenger_count, rate_code \n",
    "vendor=pd.get_dummies(data['vendor_id'], prefix = 'vendorid')\n",
    "days=pd.get_dummies(data['weekday'],prefix='Day')\n",
    "pay=pd.get_dummies(data['payment_type'], prefix='Pay')\n",
    "\n",
    "# Exploring feature cross between vendor and payment\n",
    "data['fcross']=data.vendor_id+\"_\"+data.payment_type\n",
    "fcross=pd.get_dummies(data['fcross'],prefix='cross')\n",
    "\n",
    "# Co ordinates transformation\n",
    "coords = np.vstack((data[['pickup_latitude', 'pickup_longitude']].values,\n",
    "                    data[['dropoff_latitude', 'dropoff_longitude']].values))\n",
    "\n",
    "sample_ind = np.random.permutation(len(coords))[:500000]\n",
    "kmeans = cluster.MiniBatchKMeans(n_clusters=10, batch_size=100).fit(coords[sample_ind])\n",
    "\n",
    "data.loc[:, 'pickup_cluster'] = kmeans.predict(data[['pickup_latitude', 'pickup_longitude']])\n",
    "data.loc[:, 'dropoff_cluster'] = kmeans.predict(data[['dropoff_latitude', 'dropoff_longitude']])\n",
    "#data=data.drop(['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude'],axis=1)\n",
    "\n",
    "pickup=pd.get_dummies(data['pickup_cluster'],prefix='Pickcluster')\n",
    "drop=pd.get_dummies(data['dropoff_cluster'],prefix='dropcluster')\n",
    "data=pd.concat([data.drop(['payment_type','weekday','vendor_id','passenger_count','rate_code','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','fcross'],axis=1),rate,pay,days,vendor,year,pickup,drop,fcross],axis = 1)\n",
    "\n",
    "print('No.of records with distance less than zero after cleaning',len(data[data.trip_distance < 0]))\n",
    "# Cleaning the records with high duration. Setting max duration time as 3 hours\n",
    "#data=data[data.duration<=10800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[data.flag == 0]\n",
    "test = data[data.flag == 1]\n",
    "train=train.drop(['flag'],axis=1)\n",
    "test=test.drop(['flag'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1071910, 77) (10828, 77)\n",
      "(1071165, 77)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, test.shape)\n",
    "train=train[train['duration'] >0]\n",
    "train=train[train.duration<=10800]\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test.drop(['fare_amount','tip_amount','tip_paid'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x245e3546358>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train.drop(['hour'],axis=1)\n",
    "sns.distplot(train.duration,hist=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fare_amount', 'tip_amount', 'tip_paid', 'trip_distance', 'duration',\n",
       "       'sec_norm', 'passenger_count_1', 'passenger_count_2',\n",
       "       'passenger_count_3', 'passenger_count_4', 'passenger_count_5',\n",
       "       'pickup_cluster', 'dropoff_cluster', 'rate_0.0', 'rate_1.0', 'rate_2.0',\n",
       "       'rate_3.0', 'rate_4.0', 'rate_5.0', 'rate_6.0', 'rate_99.0',\n",
       "       'rate_100.0', 'Pay_CRD', 'Pay_CSH', 'Pay_DIS', 'Pay_NA ', 'Pay_NOC',\n",
       "       'Pay_UNK', 'Day_0', 'Day_1', 'Day_2', 'Day_3', 'Day_4', 'Day_5',\n",
       "       'Day_6', 'vendorid_CMT', 'vendorid_DDS', 'vendorid_VTS', 'year=_2008',\n",
       "       'year=_2009', 'year=_2010', 'year=_2011', 'year=_2012', 'year=_2013',\n",
       "       'year=_2014', 'year=_2015', 'Pickcluster_0', 'Pickcluster_1',\n",
       "       'Pickcluster_2', 'Pickcluster_3', 'Pickcluster_4', 'Pickcluster_5',\n",
       "       'Pickcluster_6', 'Pickcluster_7', 'Pickcluster_8', 'Pickcluster_9',\n",
       "       'dropcluster_0', 'dropcluster_1', 'dropcluster_2', 'dropcluster_3',\n",
       "       'dropcluster_4', 'dropcluster_5', 'dropcluster_6', 'dropcluster_7',\n",
       "       'dropcluster_8', 'dropcluster_9', 'cross_CMT_CRD', 'cross_CMT_CSH',\n",
       "       'cross_CMT_DIS', 'cross_CMT_NA ', 'cross_CMT_NOC', 'cross_CMT_UNK',\n",
       "       'cross_DDS_CRD', 'cross_DDS_CSH', 'cross_VTS_CRD', 'cross_VTS_CSH',\n",
       "       'cross_VTS_NA '],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model for fare with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Regression tree: \n",
      "MSE on train data : 5.455508721812232e-06\n",
      "MSE on test data : 1.4977664239402895\n"
     ]
    }
   ],
   "source": [
    "# Model building for Fare amount\n",
    "data=train[['trip_distance','duration','sec_norm',\n",
    "       'passenger_count_1', 'passenger_count_2', 'passenger_count_3',\n",
    "       'passenger_count_4', 'passenger_count_5', 'rate_0.0', 'rate_1.0',\n",
    "       'rate_2.0', 'rate_3.0', 'rate_4.0', 'rate_5.0', 'rate_6.0', 'rate_99.0',\n",
    "       'rate_100.0', 'Pay_CRD', 'Pay_CSH', 'Pay_DIS', 'Pay_NA ', 'Pay_NOC',\n",
    "       'Pay_UNK', 'Day_0', 'Day_1', 'Day_2', 'Day_3', 'Day_4', 'Day_5',\n",
    "       'Day_6', 'vendorid_CMT', 'vendorid_DDS', 'vendorid_VTS', 'year=_2008',\n",
    "       'year=_2009', 'year=_2010', 'year=_2011', 'year=_2012', 'year=_2013',\n",
    "       'year=_2014', 'year=_2015', 'Pickcluster_0', 'Pickcluster_1',\n",
    "       'Pickcluster_2', 'Pickcluster_3', 'Pickcluster_4', 'Pickcluster_5',\n",
    "       'Pickcluster_6', 'Pickcluster_7', 'Pickcluster_8', 'Pickcluster_9',\n",
    "       'dropcluster_0', 'dropcluster_1', 'dropcluster_2', 'dropcluster_3',\n",
    "       'dropcluster_4', 'dropcluster_5', 'dropcluster_6', 'dropcluster_7',\n",
    "       'dropcluster_8', 'dropcluster_9','cross_CMT_CRD', 'cross_CMT_CSH',\n",
    "       'cross_CMT_DIS', 'cross_CMT_NA ', 'cross_CMT_NOC', 'cross_CMT_UNK',\n",
    "       'cross_DDS_CRD', 'cross_DDS_CSH', 'cross_VTS_CRD', 'cross_VTS_CSH',\n",
    "       'cross_VTS_NA ']].values\n",
    "y_fare=train[['fare_amount']].values\n",
    "\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(data, y_fare, test_size = 0.2, random_state = 2019)\n",
    "dt_reg_estimator = tree.DecisionTreeRegressor()\n",
    "dt_reg_estimator.fit(x_train, y_train)\n",
    "y_fare_pred=dt_reg_estimator.predict(x_train)\n",
    "print('Metrics for Regression tree: ')\n",
    "print('MSE on train data : {}'.format(mean_squared_error(y_train, y_fare_pred)))\n",
    "\n",
    "y_fare_pred_test=dt_reg_estimator.predict(x_test)\n",
    "print('MSE on test data : {}'.format(mean_squared_error(y_test, y_fare_pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Square error approximately = 0 on training set. This is a sign of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base model for tip amount with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on training set : 1.8115498079194146e-05\n",
      "MSE on test set : 1.0398325445893022\n"
     ]
    }
   ],
   "source": [
    "y_tipamount=train[['tip_amount']].values\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(data, y_tipamount, test_size = 0.2, \n",
    "                                                                    random_state = 2019)\n",
    "dt_reg_estimator.fit(x_train, y_train)\n",
    "y_tipamount_pred=dt_reg_estimator.predict(x_train)\n",
    "print('MSE on training set : {}'.format(mean_squared_error(y_train, y_tipamount_pred)))\n",
    "\n",
    "y_tipamount_pred_test=dt_reg_estimator.predict(x_test)\n",
    "print('MSE on test set : {}'.format(mean_squared_error(y_test, y_tipamount_pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Square error approximately = 0 on training set. This is sign of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base model for tip paid with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.9999976660925254\n",
      "Confusion Matrix on training set: [[494696      0]\n",
      " [     2 362234]]\n",
      "\n",
      "Area under the curve for test set: 0.9999972393688092\n",
      "Recall score for training set: 0.9999944787376186\n",
      "Precision score for training set: 1.0\n",
      "Accuracy on test set: 0.974905826833401\n",
      "Confusion Matrix on test set: [[121179   2343]\n",
      " [  3033  87678]]\n",
      "\n",
      "Area under the curve for test set: 0.9737979311570906\n",
      "Recall score for test set: 0.9665641432681814\n",
      "Precision score for test set: 0.9739727396940714\n"
     ]
    }
   ],
   "source": [
    "y_tip_paid=train[['tip_paid']].values\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(data, y_tip_paid, test_size = 0.2, \n",
    "                                                                    random_state = 2019)\n",
    "dt_estimator=tree.DecisionTreeClassifier()\n",
    "dt_estimator.fit(x_train,y_train)\n",
    "y_tip_paid_pred=dt_estimator.predict(x_train)\n",
    "\n",
    "print('Accuracy on training set: {}'.format(accuracy_score(y_train, y_tip_paid_pred)))\n",
    "print('Confusion Matrix on training set: {}\\n'.format(confusion_matrix(y_train, y_tip_paid_pred)))\n",
    "print('Area under the curve for test set: {}'.format(roc_auc_score(y_train, y_tip_paid_pred)))\n",
    "print('Recall score for training set: {}'.format(recall_score(y_train, y_tip_paid_pred)))\n",
    "print('Precision score for training set: {}'.format(precision_score(y_train, y_tip_paid_pred)))\n",
    "\n",
    "y_tip_paid_pred_test=dt_estimator.predict(x_test)\n",
    "print('Accuracy on test set: {}'.format(accuracy_score(y_test, y_tip_paid_pred_test)))\n",
    "print('Confusion Matrix on test set: {}\\n'.format(confusion_matrix(y_test,y_tip_paid_pred_test)))\n",
    "print('Area under the curve for test set: {}'.format(roc_auc_score(y_test, y_tip_paid_pred_test)))\n",
    "print('Recall score for test set: {}'.format(recall_score(y_test, y_tip_paid_pred_test)))\n",
    "print('Precision score for test set: {}'.format(precision_score(y_test, y_tip_paid_pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy = 1 on training data. This is a sign of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Hyper parametertuning (Max depth and min_samples_leaf) for fare amount "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set\n",
      "RMSE for depth of depth = 7 and min samples in the leaf = 7: 1.1608923125509107\n",
      "Test set\n",
      "RMSE for depth of depth = 7 and min samples in the leaf = 7: 1.2238326780815625\n",
      "Training set\n",
      "RMSE for depth of depth = 7 and min samples in the leaf = 5: 1.1521129891381918\n",
      "Test set\n",
      "RMSE for depth of depth = 7 and min samples in the leaf = 5: 1.2238326780815625\n",
      "Training set\n",
      "RMSE for depth of depth = 7 and min samples in the leaf = 3: 1.1521129891381918\n",
      "Test set\n",
      "RMSE for depth of depth = 7 and min samples in the leaf = 3: 1.2238326780815625\n",
      "Training set\n",
      "RMSE for depth of depth = 5 and min samples in the leaf = 7: 1.382306076431375\n",
      "Test set\n",
      "RMSE for depth of depth = 5 and min samples in the leaf = 7: 1.2238326780815625\n",
      "Training set\n",
      "RMSE for depth of depth = 5 and min samples in the leaf = 5: 1.382306076431375\n",
      "Test set\n",
      "RMSE for depth of depth = 5 and min samples in the leaf = 5: 1.2238326780815625\n",
      "Training set\n",
      "RMSE for depth of depth = 5 and min samples in the leaf = 3: 1.3823060764313748\n",
      "Test set\n",
      "RMSE for depth of depth = 5 and min samples in the leaf = 3: 1.2238326780815625\n",
      "Training set\n",
      "RMSE for depth of depth = 3 and min samples in the leaf = 7: 1.8151993447090005\n",
      "Test set\n",
      "RMSE for depth of depth = 3 and min samples in the leaf = 7: 1.2238326780815625\n",
      "Training set\n",
      "RMSE for depth of depth = 3 and min samples in the leaf = 5: 1.8151993447090002\n",
      "Test set\n",
      "RMSE for depth of depth = 3 and min samples in the leaf = 5: 1.2238326780815625\n",
      "Training set\n",
      "RMSE for depth of depth = 3 and min samples in the leaf = 3: 1.815199344709\n",
      "Test set\n",
      "RMSE for depth of depth = 3 and min samples in the leaf = 3: 1.2238326780815625\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(data, y_fare, test_size = 0.2, \n",
    "                                                                    random_state = 2019)\n",
    "\n",
    "dt_reg_estimator = tree.DecisionTreeRegressor(max_depth = 7,min_samples_leaf = 7)\n",
    "dt_reg_estimator.fit(x_train, y_train)\n",
    "y_fare_pred=dt_reg_estimator.predict(x_train)\n",
    "y_fare_pred_pred=dt_reg_estimator.predict(x_test)\n",
    "print('Training set')\n",
    "print('RMSE for depth of depth = 7 and min samples in the leaf = 7: {}'\n",
    "      .format(sqrt(mean_squared_error(y_train, y_fare_pred))))\n",
    "print('Test set')\n",
    "print('RMSE for depth of depth = 7 and min samples in the leaf = 7: {}'\n",
    "      .format(sqrt(mean_squared_error(y_test, y_fare_pred_test))))\n",
    "\n",
    "dt_reg_estimator = tree.DecisionTreeRegressor(max_depth = 7,min_samples_leaf = 3)\n",
    "dt_reg_estimator.fit(x_train, y_train)\n",
    "y_fare_pred=dt_reg_estimator.predict(x_train)\n",
    "y_fare_pred_pred=dt_reg_estimator.predict(x_test)\n",
    "print('Training set')\n",
    "print('RMSE for depth of depth = 7 and min samples in the leaf = 5: {}'\n",
    "      .format(sqrt(mean_squared_error(y_train, y_fare_pred))))\n",
    "print('Test set')\n",
    "print('RMSE for depth of depth = 7 and min samples in the leaf = 5: {}'\n",
    "      .format(sqrt(mean_squared_error(y_test, y_fare_pred_test))))\n",
    "\n",
    "dt_reg_estimator = tree.DecisionTreeRegressor(max_depth = 7,min_samples_leaf = 3)\n",
    "dt_reg_estimator.fit(x_train, y_train)\n",
    "y_fare_pred=dt_reg_estimator.predict(x_train)\n",
    "y_fare_pred_pred=dt_reg_estimator.predict(x_test)\n",
    "print('Training set')\n",
    "print('RMSE for depth of depth = 7 and min samples in the leaf = 3: {}'\n",
    "      .format(sqrt(mean_squared_error(y_train, y_fare_pred))))\n",
    "print('Test set')\n",
    "print('RMSE for depth of depth = 7 and min samples in the leaf = 3: {}'\n",
    "      .format(sqrt(mean_squared_error(y_test, y_fare_pred_test))))\n",
    "\n",
    "dt_reg_estimator = tree.DecisionTreeRegressor(max_depth = 5,min_samples_leaf = 7)\n",
    "dt_reg_estimator.fit(x_train, y_train)\n",
    "y_fare_pred=dt_reg_estimator.predict(x_train)\n",
    "y_fare_pred_pred=dt_reg_estimator.predict(x_test)\n",
    "print('Training set')\n",
    "print('RMSE for depth of depth = 5 and min samples in the leaf = 7: {}'\n",
    "      .format(sqrt(mean_squared_error(y_train, y_fare_pred))))\n",
    "print('Test set')\n",
    "print('RMSE for depth of depth = 5 and min samples in the leaf = 7: {}'\n",
    "      .format(sqrt(mean_squared_error(y_test, y_fare_pred_test))))\n",
    "\n",
    "dt_reg_estimator = tree.DecisionTreeRegressor(max_depth = 5,min_samples_leaf = 5)\n",
    "dt_reg_estimator.fit(x_train, y_train)\n",
    "y_fare_pred=dt_reg_estimator.predict(x_train)\n",
    "y_fare_pred_pred=dt_reg_estimator.predict(x_test)\n",
    "print('Training set')\n",
    "print('RMSE for depth of depth = 5 and min samples in the leaf = 5: {}'\n",
    "      .format(sqrt(mean_squared_error(y_train, y_fare_pred))))\n",
    "print('Test set')\n",
    "print('RMSE for depth of depth = 5 and min samples in the leaf = 5: {}'\n",
    "      .format(sqrt(mean_squared_error(y_test, y_fare_pred_test))))\n",
    "\n",
    "\n",
    "dt_reg_estimator = tree.DecisionTreeRegressor(max_depth = 5,min_samples_leaf = 3)\n",
    "dt_reg_estimator.fit(x_train, y_train)\n",
    "y_fare_pred=dt_reg_estimator.predict(x_train)\n",
    "y_fare_pred_pred=dt_reg_estimator.predict(x_test)\n",
    "print('Training set')\n",
    "print('RMSE for depth of depth = 5 and min samples in the leaf = 3: {}'\n",
    "      .format(sqrt(mean_squared_error(y_train, y_fare_pred))))\n",
    "print('Test set')\n",
    "print('RMSE for depth of depth = 5 and min samples in the leaf = 3: {}'\n",
    "      .format(sqrt(mean_squared_error(y_test, y_fare_pred_test))))\n",
    "\n",
    "dt_reg_estimator = tree.DecisionTreeRegressor(max_depth = 3,min_samples_leaf = 7)\n",
    "dt_reg_estimator.fit(x_train, y_train)\n",
    "y_fare_pred=dt_reg_estimator.predict(x_train)\n",
    "y_fare_pred_pred=dt_reg_estimator.predict(x_test)\n",
    "print('Training set')\n",
    "print('RMSE for depth of depth = 3 and min samples in the leaf = 7: {}'\n",
    "      .format(sqrt(mean_squared_error(y_train, y_fare_pred))))\n",
    "print('Test set')\n",
    "print('RMSE for depth of depth = 3 and min samples in the leaf = 7: {}'\n",
    "      .format(sqrt(mean_squared_error(y_test, y_fare_pred_test))))\n",
    "\n",
    "\n",
    "dt_reg_estimator = tree.DecisionTreeRegressor(max_depth = 3,min_samples_leaf = 5)\n",
    "dt_reg_estimator.fit(x_train, y_train)\n",
    "y_fare_pred=dt_reg_estimator.predict(x_train)\n",
    "y_fare_pred_pred=dt_reg_estimator.predict(x_test)\n",
    "print('Training set')\n",
    "print('RMSE for depth of depth = 3 and min samples in the leaf = 5: {}'\n",
    "      .format(sqrt(mean_squared_error(y_train, y_fare_pred))))\n",
    "print('Test set')\n",
    "print('RMSE for depth of depth = 3 and min samples in the leaf = 5: {}'\n",
    "      .format(sqrt(mean_squared_error(y_test, y_fare_pred_test))))\n",
    "\n",
    "\n",
    "dt_reg_estimator = tree.DecisionTreeRegressor(max_depth = 3,min_samples_leaf = 3)\n",
    "dt_reg_estimator.fit(x_train, y_train)\n",
    "y_fare_pred=dt_reg_estimator.predict(x_train)\n",
    "y_fare_pred_pred=dt_reg_estimator.predict(x_test)\n",
    "print('Training set')\n",
    "print('RMSE for depth of depth = 3 and min samples in the leaf = 3: {}'\n",
    "      .format(sqrt(mean_squared_error(y_train, y_fare_pred))))\n",
    "print('Test set')\n",
    "print('RMSE for depth of depth = 3 and min samples in the leaf = 3: {}'\n",
    "      .format(sqrt(mean_squared_error(y_test, y_fare_pred_test))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Hyper parametertuning (Max depth and min_samples_leaf) for tip amount "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set\n",
      "RMSE for depth of depth = 7 and min samples in the leaf = 7: 0.6504805245049889\n",
      "Test set\n",
      "RMSE for depth of depth = 7 and min samples in the leaf = 7: 0.7578377996048383\n",
      "Training set\n",
      "RMSE for depth of depth = 7 and min samples in the leaf = 5: 0.6491618697809148\n",
      "Test set\n",
      "RMSE for depth of depth = 7 and min samples in the leaf = 5: 0.7593968075137407\n",
      "Training set\n",
      "RMSE for depth of depth = 7 and min samples in the leaf = 3: 0.6491618697809148\n",
      "Test set\n",
      "RMSE for depth of depth = 7 and min samples in the leaf = 3: 0.7593971489992353\n",
      "Training set\n",
      "RMSE for depth of depth = 5 and min samples in the leaf = 7: 0.6596162680388729\n",
      "Test set\n",
      "RMSE for depth of depth = 5 and min samples in the leaf = 7: 0.7625468062373366\n",
      "Training set\n",
      "RMSE for depth of depth = 5 and min samples in the leaf = 5: 0.6596126537331908\n",
      "Test set\n",
      "RMSE for depth of depth = 5 and min samples in the leaf = 5: 0.7625457072494524\n",
      "Training set\n",
      "RMSE for depth of depth = 5 and min samples in the leaf = 3: 0.6596022480590726\n",
      "Test set\n",
      "RMSE for depth of depth = 5 and min samples in the leaf = 3: 0.7625517001118346\n",
      "Training set\n",
      "RMSE for depth of depth = 3 and min samples in the leaf = 7: 0.6802477474371165\n",
      "Test set\n",
      "RMSE for depth of depth = 3 and min samples in the leaf = 7: 0.7804449241056935\n",
      "Training set\n",
      "RMSE for depth of depth = 3 and min samples in the leaf = 5: 0.6802477474371165\n",
      "Test set\n",
      "RMSE for depth of depth = 3 and min samples in the leaf = 5: 0.7804449241056934\n",
      "Training set\n",
      "RMSE for depth of depth = 3 and min samples in the leaf = 3: 0.6802477474371164\n",
      "Test set\n",
      "RMSE for depth of depth = 3 and min samples in the leaf = 3: 0.7804449241056934\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(data, y_tipamount, test_size = 0.2, \n",
    "                                                                    random_state = 2019)\n",
    "\n",
    "dt_reg_estimator = tree.DecisionTreeRegressor(max_depth = 7,min_samples_leaf = 7)\n",
    "dt_reg_estimator.fit(x_train, y_train)\n",
    "y_tip_pred=dt_reg_estimator.predict(x_train)\n",
    "y_tip_pred_test=dt_reg_estimator.predict(x_test)\n",
    "print('Training set')\n",
    "print('RMSE for depth of depth = 7 and min samples in the leaf = 7: {}'\n",
    "      .format(sqrt(mean_squared_error(y_train, y_tip_pred))))\n",
    "print('Test set')\n",
    "print('RMSE for depth of depth = 7 and min samples in the leaf = 7: {}'\n",
    "      .format(sqrt(mean_squared_error(y_test, y_tip_pred_test))))\n",
    "\n",
    "dt_reg_estimator = tree.DecisionTreeRegressor(max_depth = 7,min_samples_leaf = 3)\n",
    "dt_reg_estimator.fit(x_train, y_train)\n",
    "y_tip_pred=dt_reg_estimator.predict(x_train)\n",
    "y_tip_pred_test=dt_reg_estimator.predict(x_test)\n",
    "print('Training set')\n",
    "print('RMSE for depth of depth = 7 and min samples in the leaf = 5: {}'\n",
    "      .format(sqrt(mean_squared_error(y_train, y_tip_pred))))\n",
    "print('Test set')\n",
    "print('RMSE for depth of depth = 7 and min samples in the leaf = 5: {}'\n",
    "      .format(sqrt(mean_squared_error(y_test, y_tip_pred_test))))\n",
    "\n",
    "dt_reg_estimator = tree.DecisionTreeRegressor(max_depth = 7,min_samples_leaf = 3)\n",
    "dt_reg_estimator.fit(x_train, y_train)\n",
    "y_tip_pred=dt_reg_estimator.predict(x_train)\n",
    "y_tip_pred_test=dt_reg_estimator.predict(x_test)\n",
    "print('Training set')\n",
    "print('RMSE for depth of depth = 7 and min samples in the leaf = 3: {}'\n",
    "      .format(sqrt(mean_squared_error(y_train, y_tip_pred))))\n",
    "print('Test set')\n",
    "print('RMSE for depth of depth = 7 and min samples in the leaf = 3: {}'\n",
    "      .format(sqrt(mean_squared_error(y_test, y_tip_pred_test))))\n",
    "\n",
    "dt_reg_estimator = tree.DecisionTreeRegressor(max_depth = 5,min_samples_leaf = 7)\n",
    "dt_reg_estimator.fit(x_train, y_train)\n",
    "y_tip_pred=dt_reg_estimator.predict(x_train)\n",
    "y_tip_pred_test=dt_reg_estimator.predict(x_test)\n",
    "print('Training set')\n",
    "print('RMSE for depth of depth = 5 and min samples in the leaf = 7: {}'\n",
    "      .format(sqrt(mean_squared_error(y_train, y_tip_pred))))\n",
    "print('Test set')\n",
    "print('RMSE for depth of depth = 5 and min samples in the leaf = 7: {}'\n",
    "      .format(sqrt(mean_squared_error(y_test, y_tip_pred_test))))\n",
    "\n",
    "dt_reg_estimator = tree.DecisionTreeRegressor(max_depth = 5,min_samples_leaf = 5)\n",
    "dt_reg_estimator.fit(x_train, y_train)\n",
    "y_tip_pred=dt_reg_estimator.predict(x_train)\n",
    "y_tip_pred_test=dt_reg_estimator.predict(x_test)\n",
    "print('Training set')\n",
    "print('RMSE for depth of depth = 5 and min samples in the leaf = 5: {}'\n",
    "      .format(sqrt(mean_squared_error(y_train, y_tip_pred))))\n",
    "print('Test set')\n",
    "print('RMSE for depth of depth = 5 and min samples in the leaf = 5: {}'\n",
    "      .format(sqrt(mean_squared_error(y_test, y_tip_pred_test))))\n",
    "\n",
    "\n",
    "dt_reg_estimator = tree.DecisionTreeRegressor(max_depth = 5,min_samples_leaf = 3)\n",
    "dt_reg_estimator.fit(x_train, y_train)\n",
    "y_tip_pred=dt_reg_estimator.predict(x_train)\n",
    "y_tip_pred_test=dt_reg_estimator.predict(x_test)\n",
    "print('Training set')\n",
    "print('RMSE for depth of depth = 5 and min samples in the leaf = 3: {}'\n",
    "      .format(sqrt(mean_squared_error(y_train, y_tip_pred))))\n",
    "print('Test set')\n",
    "print('RMSE for depth of depth = 5 and min samples in the leaf = 3: {}'\n",
    "      .format(sqrt(mean_squared_error(y_test, y_tip_pred_test))))\n",
    "\n",
    "dt_reg_estimator = tree.DecisionTreeRegressor(max_depth = 3,min_samples_leaf = 7)\n",
    "dt_reg_estimator.fit(x_train, y_train)\n",
    "y_tip_pred=dt_reg_estimator.predict(x_train)\n",
    "y_tip_pred_test=dt_reg_estimator.predict(x_test)\n",
    "print('Training set')\n",
    "print('RMSE for depth of depth = 3 and min samples in the leaf = 7: {}'\n",
    "      .format(sqrt(mean_squared_error(y_train, y_tip_pred))))\n",
    "print('Test set')\n",
    "print('RMSE for depth of depth = 3 and min samples in the leaf = 7: {}'\n",
    "      .format(sqrt(mean_squared_error(y_test, y_tip_pred_test))))\n",
    "\n",
    "\n",
    "dt_reg_estimator = tree.DecisionTreeRegressor(max_depth = 3,min_samples_leaf = 5)\n",
    "dt_reg_estimator.fit(x_train, y_train)\n",
    "y_tip_pred=dt_reg_estimator.predict(x_train)\n",
    "y_tip_pred_test=dt_reg_estimator.predict(x_test)\n",
    "print('Training set')\n",
    "print('RMSE for depth of depth = 3 and min samples in the leaf = 5: {}'\n",
    "      .format(sqrt(mean_squared_error(y_train, y_tip_pred))))\n",
    "print('Test set')\n",
    "print('RMSE for depth of depth = 3 and min samples in the leaf = 5: {}'\n",
    "      .format(sqrt(mean_squared_error(y_test, y_tip_pred_test))))\n",
    "\n",
    "\n",
    "dt_reg_estimator = tree.DecisionTreeRegressor(max_depth = 3,min_samples_leaf = 3)\n",
    "dt_reg_estimator.fit(x_train, y_train)\n",
    "y_tip_pred=dt_reg_estimator.predict(x_train)\n",
    "y_tip_pred_test=dt_reg_estimator.predict(x_test)\n",
    "print('Training set')\n",
    "print('RMSE for depth of depth = 3 and min samples in the leaf = 3: {}'\n",
    "      .format(sqrt(mean_squared_error(y_train, y_tip_pred))))\n",
    "print('Test set')\n",
    "print('RMSE for depth of depth = 3 and min samples in the leaf = 3: {}'\n",
    "      .format(sqrt(mean_squared_error(y_test, y_tip_pred_test))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Hyper parametertuning (criterion and depth of the tree) for tip paid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Criterion=entropy and max_depth=3\n",
      "Training set\n",
      "Accuracy: 0.9886863835170119\n",
      "Area under the curve for test set: 0.9901474616581792\n",
      "Recall score for training set: 0.9995997084773462\n",
      "Precision score for training set: 0.9743031581553165\n",
      "\n",
      "Criterion=entropy and max_depth=5\n",
      "Training set\n",
      "Accuracy: 0.9886875504707492\n",
      "Area under the curve for test set: 0.9901488419737746\n",
      "Recall score for training set: 0.999602469108537\n",
      "Precision score for training set: 0.9743032272993902\n",
      "Test set\n",
      "Accuracy: 0.9887832406772066\n",
      "Area under the curve for test set: 0.9902305274940343\n",
      "Recall score for test set: 0.9996803033810674\n",
      "Precision score for test set: 0.9744884800550206\n",
      "\n",
      "Criterion=entropy and max_depth=7\n",
      "\n",
      "Training set\n",
      "Accuracy: 0.9887050547768085\n",
      "Area under the curve for test set: 0.990165481175254\n",
      "Recall score for training set: 0.9996135116332998\n",
      "Precision score for training set: 0.9743323422174983\n",
      "Test set\n",
      "Accuracy: 0.9887692372323592\n",
      "Area under the curve for test set: 0.9902169197595244\n",
      "Recall score for test set: 0.9996692793597248\n",
      "Precision score for test set: 0.9744672620007092\n",
      "\n",
      "Criterion=entropy and max_depth=9\n",
      "\n",
      "Training set\n",
      "Accuracy: 0.9887517329262999\n",
      "Area under the curve for test set: 0.9902062796385602\n",
      "Recall score for training set: 0.9996162722644906\n",
      "Precision score for training set: 0.974434669817033\n",
      "Test set\n",
      "Accuracy: 0.9887038878230712\n",
      "Area under the curve for test set: 0.9901456082051344\n",
      "Recall score for test set: 0.9995590391462997\n",
      "Precision score for test set: 0.9744226284510644\n",
      "\n",
      "Criterion=entropy and max_depth=11\n",
      "\n",
      "Training set\n",
      "Accuracy: 0.9888439222715455\n",
      "Area under the curve for test set: 0.9902839090925633\n",
      "Recall score for training set: 0.9995997084773462\n",
      "Precision score for training set: 0.9746572060747336\n",
      "Test set\n",
      "Accuracy: 0.9885731890044951\n",
      "Area under the curve for test set: 0.9899956643525951\n",
      "Recall score for test set: 0.9992834386127372\n",
      "Precision score for test set: 0.9743843317675134\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(data, y_tip_paid, test_size = 0.2, \n",
    "                                                                    random_state = 2019)\n",
    "\n",
    "dt_estimator=tree.DecisionTreeClassifier(criterion=\"entropy\",max_depth=3)\n",
    "dt_estimator.fit(x_train,y_train)\n",
    "y_tip_paid_pred=dt_estimator.predict(x_train)\n",
    "y_tip_paid_pred_test=dt_estimator.predict(x_test)\n",
    "print('\\nCriterion=entropy and max_depth=3')\n",
    "print('Training set')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_train, y_tip_paid_pred)))\n",
    "print('Area under the curve for test set: {}'.format(roc_auc_score(y_train, y_tip_paid_pred)))\n",
    "print('Recall score for training set: {}'.format(recall_score(y_train, y_tip_paid_pred)))\n",
    "print('Precision score for training set: {}'.format(precision_score(y_train, y_tip_paid_pred)))\n",
    "\n",
    "\n",
    "dt_estimator=tree.DecisionTreeClassifier(criterion=\"entropy\",max_depth=5)\n",
    "dt_estimator.fit(x_train,y_train)\n",
    "y_tip_paid_pred=dt_estimator.predict(x_train)\n",
    "y_tip_paid_pred_test=dt_estimator.predict(x_test)\n",
    "print('\\nCriterion=entropy and max_depth=5')\n",
    "print('Training set')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_train, y_tip_paid_pred)))\n",
    "print('Area under the curve for test set: {}'.format(roc_auc_score(y_train, y_tip_paid_pred)))\n",
    "print('Recall score for training set: {}'.format(recall_score(y_train, y_tip_paid_pred)))\n",
    "print('Precision score for training set: {}'.format(precision_score(y_train, y_tip_paid_pred)))\n",
    "\n",
    "print('Test set')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, y_tip_paid_pred_test)))\n",
    "print('Area under the curve for test set: {}'.format(roc_auc_score(y_test, y_tip_paid_pred_test)))\n",
    "print('Recall score for test set: {}'.format(recall_score(y_test, y_tip_paid_pred_test)))\n",
    "print('Precision score for test set: {}'.format(precision_score(y_test, y_tip_paid_pred_test)))\n",
    "\n",
    "dt_estimator=tree.DecisionTreeClassifier(criterion=\"entropy\",max_depth=7)\n",
    "dt_estimator.fit(x_train,y_train)\n",
    "y_tip_paid_pred=dt_estimator.predict(x_train)\n",
    "y_tip_paid_pred_test=dt_estimator.predict(x_test)\n",
    "print('\\nCriterion=entropy and max_depth=7\\n')\n",
    "print('Training set')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_train, y_tip_paid_pred)))\n",
    "print('Area under the curve for test set: {}'.format(roc_auc_score(y_train, y_tip_paid_pred)))\n",
    "print('Recall score for training set: {}'.format(recall_score(y_train, y_tip_paid_pred)))\n",
    "print('Precision score for training set: {}'.format(precision_score(y_train, y_tip_paid_pred)))\n",
    "\n",
    "print('Test set')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, y_tip_paid_pred_test)))\n",
    "print('Area under the curve for test set: {}'.format(roc_auc_score(y_test, y_tip_paid_pred_test)))\n",
    "print('Recall score for test set: {}'.format(recall_score(y_test, y_tip_paid_pred_test)))\n",
    "print('Precision score for test set: {}'.format(precision_score(y_test, y_tip_paid_pred_test)))\n",
    "\n",
    "dt_estimator=tree.DecisionTreeClassifier(criterion=\"entropy\",max_depth=9)\n",
    "dt_estimator.fit(x_train,y_train)\n",
    "y_tip_paid_pred=dt_estimator.predict(x_train)\n",
    "y_tip_paid_pred_test=dt_estimator.predict(x_test)\n",
    "print('\\nCriterion=entropy and max_depth=9\\n')\n",
    "print('Training set')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_train, y_tip_paid_pred)))\n",
    "print('Area under the curve for test set: {}'.format(roc_auc_score(y_train, y_tip_paid_pred)))\n",
    "print('Recall score for training set: {}'.format(recall_score(y_train, y_tip_paid_pred)))\n",
    "print('Precision score for training set: {}'.format(precision_score(y_train, y_tip_paid_pred)))\n",
    "\n",
    "print('Test set')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, y_tip_paid_pred_test)))\n",
    "print('Area under the curve for test set: {}'.format(roc_auc_score(y_test, y_tip_paid_pred_test)))\n",
    "print('Recall score for test set: {}'.format(recall_score(y_test, y_tip_paid_pred_test)))\n",
    "print('Precision score for test set: {}'.format(precision_score(y_test, y_tip_paid_pred_test)))\n",
    "\n",
    "dt_estimator=tree.DecisionTreeClassifier(criterion=\"entropy\",max_depth=11)\n",
    "dt_estimator.fit(x_train,y_train)\n",
    "y_tip_paid_pred=dt_estimator.predict(x_train)\n",
    "y_tip_paid_pred_test=dt_estimator.predict(x_test)\n",
    "print('\\nCriterion=entropy and max_depth=11\\n')\n",
    "print('Training set')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_train, y_tip_paid_pred)))\n",
    "print('Area under the curve for test set: {}'.format(roc_auc_score(y_train, y_tip_paid_pred)))\n",
    "print('Recall score for training set: {}'.format(recall_score(y_train, y_tip_paid_pred)))\n",
    "print('Precision score for training set: {}'.format(precision_score(y_train, y_tip_paid_pred)))\n",
    "\n",
    "print('Test set')\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, y_tip_paid_pred_test)))\n",
    "print('Area under the curve for test set: {}'.format(roc_auc_score(y_test, y_tip_paid_pred_test)))\n",
    "print('Recall score for test set: {}'.format(recall_score(y_test, y_tip_paid_pred_test)))\n",
    "print('Precision score for test set: {}'.format(precision_score(y_test, y_tip_paid_pred_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest Regressor/ RandomForest Classifier with Grid SearchCV on fare , tip amount and tip paid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(data, y_fare, test_size = 0.2, \n",
    "                                                                    random_state = 2019)\n",
    "#rforestreg=ensemble.RandomForestRegressor()\n",
    "parameters=[\n",
    "    {'n_estimators':[13,15],\n",
    "#     'max_depth':[5,7,10],\n",
    "     'max_features':['sqrt','log2']\n",
    "    }\n",
    "#    {'n_estimators':[5,7,10],\n",
    "#     'max_depth':[8,9,10],\n",
    "#     'min_samples_split ':[2,3,4]\n",
    "#    }\n",
    "]\n",
    "reg = model_selection.GridSearchCV(ensemble.RandomForestRegressor(n_jobs=-1), \n",
    "                                   parameters, \n",
    "                                   cv=model_selection.KFold(n_splits=3, shuffle=True, random_state=2019),\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance:\n",
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0       5.979179      0.095752         0.572925        0.029451   \n",
      "1       6.588624      0.072503         0.645758        0.051618   \n",
      "2       5.213706      0.040981         0.541515        0.032070   \n",
      "3       5.854178      0.038956         0.666632        0.051546   \n",
      "\n",
      "  param_max_features param_n_estimators  \\\n",
      "0               sqrt                 13   \n",
      "1               sqrt                 15   \n",
      "2               log2                 13   \n",
      "3               log2                 15   \n",
      "\n",
      "                                         params  split0_test_score  \\\n",
      "0  {'max_features': 'sqrt', 'n_estimators': 13}           0.933578   \n",
      "1  {'max_features': 'sqrt', 'n_estimators': 15}           0.932421   \n",
      "2  {'max_features': 'log2', 'n_estimators': 13}           0.921758   \n",
      "3  {'max_features': 'log2', 'n_estimators': 15}           0.922189   \n",
      "\n",
      "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
      "0           0.944766           0.934865         0.937736        0.004998   \n",
      "1           0.947718           0.938255         0.939465        0.006303   \n",
      "2           0.932113           0.922619         0.925497        0.004692   \n",
      "3           0.937311           0.928290         0.929263        0.006211   \n",
      "\n",
      "   rank_test_score  split0_train_score  split1_train_score  \\\n",
      "0                2            0.988536            0.988565   \n",
      "1                1            0.989885            0.987976   \n",
      "2                4            0.986650            0.984764   \n",
      "3                3            0.987928            0.984657   \n",
      "\n",
      "   split2_train_score  mean_train_score  std_train_score  \n",
      "0            0.988731          0.988611         0.000086  \n",
      "1            0.989405          0.989088         0.000811  \n",
      "2            0.986776          0.986063         0.000920  \n",
      "3            0.987774          0.986787         0.001507  \n",
      "*********************************************************************************************************************\n",
      "best parameters:  {'max_features': 'sqrt', 'n_estimators': 15}\n",
      "*********************************************************************************************************************\n",
      "best score:  0.9394646475271861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdevasish\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\mdevasish\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\mdevasish\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\mdevasish\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\mdevasish\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "reg.fit(x_train, y_train.ravel())\n",
    "print('Feature Importance:')\n",
    "print(pd.DataFrame(reg.cv_results_))\n",
    "print(117*'*')\n",
    "print('best parameters: ', reg.best_params_)\n",
    "print(117*'*')\n",
    "print('best score: ',reg.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='sqrt', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train=train[['trip_distance','duration','sec_norm',\n",
    "       'passenger_count_1', 'passenger_count_2', 'passenger_count_3',\n",
    "       'passenger_count_4', 'passenger_count_5', 'rate_0.0', 'rate_1.0',\n",
    "       'rate_2.0', 'rate_3.0', 'rate_4.0', 'rate_5.0', 'rate_6.0', 'rate_99.0',\n",
    "       'rate_100.0', 'Pay_CRD', 'Pay_CSH', 'Pay_DIS', 'Pay_NA ', 'Pay_NOC',\n",
    "       'Pay_UNK', 'Day_0', 'Day_1', 'Day_2', 'Day_3', 'Day_4', 'Day_5',\n",
    "       'Day_6', 'vendorid_CMT', 'vendorid_DDS', 'vendorid_VTS', 'year=_2008',\n",
    "       'year=_2009', 'year=_2010', 'year=_2011', 'year=_2012', 'year=_2013',\n",
    "       'year=_2014', 'year=_2015', 'Pickcluster_0', 'Pickcluster_1',\n",
    "       'Pickcluster_2', 'Pickcluster_3', 'Pickcluster_4', 'Pickcluster_5',\n",
    "       'Pickcluster_6', 'Pickcluster_7', 'Pickcluster_8', 'Pickcluster_9',\n",
    "       'dropcluster_0', 'dropcluster_1', 'dropcluster_2', 'dropcluster_3',\n",
    "       'dropcluster_4', 'dropcluster_5', 'dropcluster_6', 'dropcluster_7',\n",
    "       'dropcluster_8', 'dropcluster_9','cross_CMT_CRD', 'cross_CMT_CSH',\n",
    "       'cross_CMT_DIS', 'cross_CMT_NA ', 'cross_CMT_NOC', 'cross_CMT_UNK',\n",
    "       'cross_DDS_CRD', 'cross_DDS_CSH', 'cross_VTS_CRD', 'cross_VTS_CSH',\n",
    "       'cross_VTS_NA ']].values\n",
    "y_train=train[['fare_amount']].values\n",
    "\n",
    "rforestreg=ensemble.RandomForestRegressor(max_features='sqrt',n_estimators=15)\n",
    "rforestreg.fit(x_train, y_train.ravel())\n",
    "#test_pred=rforestreg.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=test[['trip_distance','duration','sec_norm',\n",
    "       'passenger_count_1', 'passenger_count_2', 'passenger_count_3',\n",
    "       'passenger_count_4', 'passenger_count_5', 'rate_0.0', 'rate_1.0',\n",
    "       'rate_2.0', 'rate_3.0', 'rate_4.0', 'rate_5.0', 'rate_6.0', 'rate_99.0',\n",
    "       'rate_100.0', 'Pay_CRD', 'Pay_CSH', 'Pay_DIS', 'Pay_NA ', 'Pay_NOC',\n",
    "       'Pay_UNK', 'Day_0', 'Day_1', 'Day_2', 'Day_3', 'Day_4', 'Day_5',\n",
    "       'Day_6', 'vendorid_CMT', 'vendorid_DDS', 'vendorid_VTS', 'year=_2008',\n",
    "       'year=_2009', 'year=_2010', 'year=_2011', 'year=_2012', 'year=_2013',\n",
    "       'year=_2014', 'year=_2015', 'Pickcluster_0', 'Pickcluster_1',\n",
    "       'Pickcluster_2', 'Pickcluster_3', 'Pickcluster_4', 'Pickcluster_5',\n",
    "       'Pickcluster_6', 'Pickcluster_7', 'Pickcluster_8', 'Pickcluster_9',\n",
    "       'dropcluster_0', 'dropcluster_1', 'dropcluster_2', 'dropcluster_3',\n",
    "       'dropcluster_4', 'dropcluster_5', 'dropcluster_6', 'dropcluster_7',\n",
    "       'dropcluster_8', 'dropcluster_9','cross_CMT_CRD', 'cross_CMT_CSH',\n",
    "       'cross_CMT_DIS', 'cross_CMT_NA ', 'cross_CMT_NOC', 'cross_CMT_UNK',\n",
    "       'cross_DDS_CRD', 'cross_DDS_CSH', 'cross_VTS_CRD', 'cross_VTS_CSH',\n",
    "       'cross_VTS_NA ']].values\n",
    "test_pred=rforestreg.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.55333333,  5.60666667, 10.79333333, ...,  9.78666667,\n",
       "        5.24666667, 13.01333333])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.DataFrame(test_pred)\n",
    "x.to_csv('MyPred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : 0.9353151194806221\n"
     ]
    }
   ],
   "source": [
    "rforestreg=ensemble.RandomForestRegressor(max_features='sqrt',n_estimators=15)\n",
    "rforestreg.fit(x_train,y_train.ravel())\n",
    "pred=rforestreg.predict(x_test)\n",
    "print('RMSE : {}'.format(sqrt(mean_squared_error(y_test, pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE on the test data is aprroximately 1. Which shows that this model has performed good enough on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214233"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imp=dict(zip(train.columns,rforestreg.feature_importances_))\n",
    "#feature_imp=sorted(imp.items(), key=lambda x:x[1],reverse=True)\n",
    "#feature_imp\n",
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(data, y_tipamount, test_size = 0.2, \n",
    "                                                                    random_state = 2019)\n",
    "#rforestreg=ensemble.RandomForestRegressor()\n",
    "parameters=[\n",
    "    {'n_estimators':[13,15],\n",
    "#     'max_depth':[5,7,10],\n",
    "     'max_features':['sqrt','log2']\n",
    "    }\n",
    "#    {'n_estimators':[5,7,10],\n",
    "#     'max_depth':[8,9,10],\n",
    "#     'min_samples_split ':[2,3,4]\n",
    "#    }\n",
    "]\n",
    "reg = model_selection.GridSearchCV(ensemble.RandomForestRegressor(n_jobs=-1), \n",
    "                                   parameters, \n",
    "                                   cv=model_selection.KFold(n_splits=3, shuffle=True, random_state=2019),\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance:\n",
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0       4.978865      0.186410         0.317680        0.007344   \n",
      "1       5.355009      0.084680         0.296868        0.000021   \n",
      "2       4.288745      0.145678         0.322814        0.007406   \n",
      "3       4.667005      0.186319         0.406098        0.055559   \n",
      "\n",
      "  param_max_features param_n_estimators  \\\n",
      "0               sqrt                 13   \n",
      "1               sqrt                 15   \n",
      "2               log2                 13   \n",
      "3               log2                 15   \n",
      "\n",
      "                                         params  split0_test_score  \\\n",
      "0  {'max_features': 'sqrt', 'n_estimators': 13}           0.662205   \n",
      "1  {'max_features': 'sqrt', 'n_estimators': 15}           0.668347   \n",
      "2  {'max_features': 'log2', 'n_estimators': 13}           0.662653   \n",
      "3  {'max_features': 'log2', 'n_estimators': 15}           0.663372   \n",
      "\n",
      "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
      "0           0.706885           0.646246         0.671779        0.025665   \n",
      "1           0.708416           0.650057         0.675607        0.024372   \n",
      "2           0.706062           0.641593         0.670103        0.026841   \n",
      "3           0.712339           0.646042         0.673918        0.028074   \n",
      "\n",
      "   rank_test_score  split0_train_score  split1_train_score  \\\n",
      "0                3            0.946040            0.949728   \n",
      "1                1            0.945397            0.943933   \n",
      "2                4            0.945553            0.942051   \n",
      "3                2            0.950215            0.943803   \n",
      "\n",
      "   split2_train_score  mean_train_score  std_train_score  \n",
      "0            0.946821          0.947530         0.001587  \n",
      "1            0.952273          0.947201         0.003636  \n",
      "2            0.941893          0.943166         0.001689  \n",
      "3            0.951563          0.948527         0.003385  \n",
      "*********************************************************************************************************************\n",
      "best parameters:  {'max_features': 'sqrt', 'n_estimators': 15}\n",
      "*********************************************************************************************************************\n",
      "best score:  0.6756068408286492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdevasish\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\mdevasish\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\mdevasish\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\mdevasish\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\mdevasish\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "reg.fit(x_train, y_train.ravel())\n",
    "print('Feature Importance:')\n",
    "print(pd.DataFrame(reg.cv_results_))\n",
    "print(117*'*')\n",
    "print('best parameters: ', reg.best_params_)\n",
    "print(117*'*')\n",
    "print('best score: ',reg.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE : 0.7721015888776419\n"
     ]
    }
   ],
   "source": [
    "rforestreg=ensemble.RandomForestRegressor(max_features='sqrt',n_estimators=15)\n",
    "rforestreg.fit(x_train,y_train.ravel())\n",
    "pred=rforestreg.predict(x_test)\n",
    "print('RMSE : {}'\n",
    "      .format(sqrt(mean_squared_error(y_test, pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE on the test data is aprroximately 1. Which shows that this model has performed good enough on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.03034206e-01, 1.06245213e-01, 7.65560447e-02, 0.00000000e+00,\n",
       "       5.75744566e-03, 4.01672944e-03, 3.03679623e-03, 3.49556498e-03,\n",
       "       6.33178433e-06, 3.57614032e-03, 3.60488405e-03, 1.08314086e-06,\n",
       "       9.78967025e-05, 7.07572119e-04, 0.00000000e+00, 2.12529271e-07,\n",
       "       1.31755528e-03, 1.87926409e-01, 2.03839003e-01, 5.25955489e-05,\n",
       "       3.77321521e-04, 5.37500166e-04, 9.60763919e-06, 3.63282750e-03,\n",
       "       6.74111316e-03, 4.24502053e-03, 3.94084763e-03, 4.35750001e-03,\n",
       "       6.07568473e-03, 2.44777317e-03, 1.37650271e-02, 1.15760148e-04,\n",
       "       1.28598272e-02, 3.31815219e-06, 2.68416661e-03, 2.94038796e-03,\n",
       "       2.78902727e-03, 4.48429153e-03, 3.38401798e-03, 3.10939613e-03,\n",
       "       2.61879083e-03, 2.99677860e-03, 2.12848230e-03, 3.03677269e-03,\n",
       "       2.96227553e-03, 2.29492578e-03, 5.55774937e-03, 1.45730874e-03,\n",
       "       1.68166617e-03, 2.82950067e-03, 3.22008257e-03, 3.00075601e-03,\n",
       "       1.72263522e-03, 2.20899512e-03, 2.69388014e-03, 6.23704241e-03,\n",
       "       2.41173918e-03, 4.84480744e-03, 4.21562138e-03, 3.27103595e-03,\n",
       "       4.60928237e-03, 4.39609295e-02, 2.78940287e-02, 9.50968617e-05,\n",
       "       2.02157799e-06, 2.87029776e-04, 1.26323696e-05, 2.57038375e-04,\n",
       "       5.71131773e-04, 3.97094285e-02, 3.91718227e-02, 2.66612820e-04])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rforestreg.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(data, y_tip_paid, test_size = 0.2, \n",
    "                                                                    random_state = 2019)\n",
    "parameters=[\n",
    "    {\n",
    "     'n_estimators':[13,15],\n",
    "#     'max_depth':[5,7,10],\n",
    "     'max_features':['sqrt','log2']\n",
    "    }\n",
    "#    {'n_estimators':[5,7,10],\n",
    "#     'max_depth':[8,9,10],\n",
    "#     'min_samples_split ':[2,3,4]\n",
    "#    }\n",
    "]\n",
    "clf = model_selection.GridSearchCV(ensemble.RandomForestClassifier(n_jobs=-1), \n",
    "                                   parameters, \n",
    "                                   cv=model_selection.StratifiedKFold(n_splits=3, shuffle=True, random_state=2019),\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance:    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0       4.978865      0.186410         0.317680        0.007344   \n",
      "1       5.355009      0.084680         0.296868        0.000021   \n",
      "2       4.288745      0.145678         0.322814        0.007406   \n",
      "3       4.667005      0.186319         0.406098        0.055559   \n",
      "\n",
      "  param_max_features param_n_estimators  \\\n",
      "0               sqrt                 13   \n",
      "1               sqrt                 15   \n",
      "2               log2                 13   \n",
      "3               log2                 15   \n",
      "\n",
      "                                         params  split0_test_score  \\\n",
      "0  {'max_features': 'sqrt', 'n_estimators': 13}           0.662205   \n",
      "1  {'max_features': 'sqrt', 'n_estimators': 15}           0.668347   \n",
      "2  {'max_features': 'log2', 'n_estimators': 13}           0.662653   \n",
      "3  {'max_features': 'log2', 'n_estimators': 15}           0.663372   \n",
      "\n",
      "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
      "0           0.706885           0.646246         0.671779        0.025665   \n",
      "1           0.708416           0.650057         0.675607        0.024372   \n",
      "2           0.706062           0.641593         0.670103        0.026841   \n",
      "3           0.712339           0.646042         0.673918        0.028074   \n",
      "\n",
      "   rank_test_score  split0_train_score  split1_train_score  \\\n",
      "0                3            0.946040            0.949728   \n",
      "1                1            0.945397            0.943933   \n",
      "2                4            0.945553            0.942051   \n",
      "3                2            0.950215            0.943803   \n",
      "\n",
      "   split2_train_score  mean_train_score  std_train_score  \n",
      "0            0.946821          0.947530         0.001587  \n",
      "1            0.952273          0.947201         0.003636  \n",
      "2            0.941893          0.943166         0.001689  \n",
      "3            0.951563          0.948527         0.003385  \n",
      "*********************************************************************************************************************\n",
      "best parameters:  {'max_features': 'sqrt', 'n_estimators': 15}\n",
      "*********************************************************************************************************************\n",
      "best score:  0.9884273197873343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdevasish\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\mdevasish\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\mdevasish\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\mdevasish\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\mdevasish\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "clf.fit(x_train, y_train.ravel())\n",
    "print('Feature Importance:', pd.DataFrame(reg.cv_results_))\n",
    "print(117*'*')\n",
    "print('best parameters: ', clf.best_params_)\n",
    "print(117*'*')\n",
    "print('best score: ',clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10828,)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train=train[['trip_distance','duration','sec_norm',\n",
    "       'passenger_count_1', 'passenger_count_2', 'passenger_count_3',\n",
    "       'passenger_count_4', 'passenger_count_5', 'rate_0.0', 'rate_1.0',\n",
    "       'rate_2.0', 'rate_3.0', 'rate_4.0', 'rate_5.0', 'rate_6.0', 'rate_99.0',\n",
    "       'rate_100.0', 'Pay_CRD', 'Pay_CSH', 'Pay_DIS', 'Pay_NA ', 'Pay_NOC',\n",
    "       'Pay_UNK', 'Day_0', 'Day_1', 'Day_2', 'Day_3', 'Day_4', 'Day_5',\n",
    "       'Day_6', 'vendorid_CMT', 'vendorid_DDS', 'vendorid_VTS', 'year=_2008',\n",
    "       'year=_2009', 'year=_2010', 'year=_2011', 'year=_2012', 'year=_2013',\n",
    "       'year=_2014', 'year=_2015', 'Pickcluster_0', 'Pickcluster_1',\n",
    "       'Pickcluster_2', 'Pickcluster_3', 'Pickcluster_4', 'Pickcluster_5',\n",
    "       'Pickcluster_6', 'Pickcluster_7', 'Pickcluster_8', 'Pickcluster_9',\n",
    "       'dropcluster_0', 'dropcluster_1', 'dropcluster_2', 'dropcluster_3',\n",
    "       'dropcluster_4', 'dropcluster_5', 'dropcluster_6', 'dropcluster_7',\n",
    "       'dropcluster_8', 'dropcluster_9','cross_CMT_CRD', 'cross_CMT_CSH',\n",
    "       'cross_CMT_DIS', 'cross_CMT_NA ', 'cross_CMT_NOC', 'cross_CMT_UNK',\n",
    "       'cross_DDS_CRD', 'cross_DDS_CSH', 'cross_VTS_CRD', 'cross_VTS_CSH',\n",
    "       'cross_VTS_NA ']].values\n",
    "y_train=train[['tip_amount']].values\n",
    "\n",
    "rforestreg=ensemble.RandomForestRegressor(max_features='sqrt',n_estimators=15)\n",
    "rforestreg.fit(x_train, y_train.ravel())\n",
    "\n",
    "#rforestclf=ensemble.RandomForestClassifier(max_features='sqrt',n_estimators=15)\n",
    "#rforestclf.fit(x_train,y_train.ravel())\n",
    "pred=rforestreg.predict(x_test)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.2       , 0.        , 0.        , ..., 1.65533333, 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.DataFrame(pred)\n",
    "x.to_csv('MyPred2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9885825246343934\n",
      "Area under the curve for test set: 0.9899891185889146\n",
      "Recall score for training set: 0.9991731983993121\n",
      "Precision score for training set: 0.9745072951498275\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {}'.format(accuracy_score(y_test, pred)))\n",
    "print('Area under the curve for test set: {}'.format(roc_auc_score(y_test, pred)))\n",
    "print('Recall score for training set: {}'.format(recall_score(y_test, pred)))\n",
    "print('Precision score for training set: {}'.format(precision_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy, Area under the curve Recall and Precision are high on the test data which is an indicator of good model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train=train[['trip_distance','duration','sec_norm',\n",
    "       'passenger_count_1', 'passenger_count_2', 'passenger_count_3',\n",
    "       'passenger_count_4', 'passenger_count_5', 'rate_0.0', 'rate_1.0',\n",
    "       'rate_2.0', 'rate_3.0', 'rate_4.0', 'rate_5.0', 'rate_6.0', 'rate_99.0',\n",
    "       'rate_100.0', 'Pay_CRD', 'Pay_CSH', 'Pay_DIS', 'Pay_NA ', 'Pay_NOC',\n",
    "       'Pay_UNK', 'Day_0', 'Day_1', 'Day_2', 'Day_3', 'Day_4', 'Day_5',\n",
    "       'Day_6', 'vendorid_CMT', 'vendorid_DDS', 'vendorid_VTS', 'year=_2008',\n",
    "       'year=_2009', 'year=_2010', 'year=_2011', 'year=_2012', 'year=_2013',\n",
    "       'year=_2014', 'year=_2015', 'Pickcluster_0', 'Pickcluster_1',\n",
    "       'Pickcluster_2', 'Pickcluster_3', 'Pickcluster_4', 'Pickcluster_5',\n",
    "       'Pickcluster_6', 'Pickcluster_7', 'Pickcluster_8', 'Pickcluster_9',\n",
    "       'dropcluster_0', 'dropcluster_1', 'dropcluster_2', 'dropcluster_3',\n",
    "       'dropcluster_4', 'dropcluster_5', 'dropcluster_6', 'dropcluster_7',\n",
    "       'dropcluster_8', 'dropcluster_9','cross_CMT_CRD', 'cross_CMT_CSH',\n",
    "       'cross_CMT_DIS', 'cross_CMT_NA ', 'cross_CMT_NOC', 'cross_CMT_UNK',\n",
    "       'cross_DDS_CRD', 'cross_DDS_CSH', 'cross_VTS_CRD', 'cross_VTS_CSH',\n",
    "       'cross_VTS_NA ']].values\n",
    "y_train=train[['tip_paid']].values\n",
    "\n",
    "rforestreg=ensemble.RandomForestRegressor(max_features='sqrt',n_estimators=15)\n",
    "rforestreg.fit(x_train, y_train.ravel())\n",
    "\n",
    "x_test=test[['trip_distance','duration','sec_norm',\n",
    "       'passenger_count_1', 'passenger_count_2', 'passenger_count_3',\n",
    "       'passenger_count_4', 'passenger_count_5', 'rate_0.0', 'rate_1.0',\n",
    "       'rate_2.0', 'rate_3.0', 'rate_4.0', 'rate_5.0', 'rate_6.0', 'rate_99.0',\n",
    "       'rate_100.0', 'Pay_CRD', 'Pay_CSH', 'Pay_DIS', 'Pay_NA ', 'Pay_NOC',\n",
    "       'Pay_UNK', 'Day_0', 'Day_1', 'Day_2', 'Day_3', 'Day_4', 'Day_5',\n",
    "       'Day_6', 'vendorid_CMT', 'vendorid_DDS', 'vendorid_VTS', 'year=_2008',\n",
    "       'year=_2009', 'year=_2010', 'year=_2011', 'year=_2012', 'year=_2013',\n",
    "       'year=_2014', 'year=_2015', 'Pickcluster_0', 'Pickcluster_1',\n",
    "       'Pickcluster_2', 'Pickcluster_3', 'Pickcluster_4', 'Pickcluster_5',\n",
    "       'Pickcluster_6', 'Pickcluster_7', 'Pickcluster_8', 'Pickcluster_9',\n",
    "       'dropcluster_0', 'dropcluster_1', 'dropcluster_2', 'dropcluster_3',\n",
    "       'dropcluster_4', 'dropcluster_5', 'dropcluster_6', 'dropcluster_7',\n",
    "       'dropcluster_8', 'dropcluster_9','cross_CMT_CRD', 'cross_CMT_CSH',\n",
    "       'cross_CMT_DIS', 'cross_CMT_NA ', 'cross_CMT_NOC', 'cross_CMT_UNK',\n",
    "       'cross_DDS_CRD', 'cross_DDS_CSH', 'cross_VTS_CRD', 'cross_VTS_CSH',\n",
    "       'cross_VTS_NA ']].values\n",
    "pred=rforestclf.predict(x_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.DataFrame(pred)\n",
    "x.to_csv('MyPred1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.09453680e-02, 1.08982569e-02, 1.21121140e-02, 0.00000000e+00,\n",
       "       8.97378538e-04, 3.37664253e-04, 1.27206285e-03, 7.91569751e-04,\n",
       "       1.42869880e-06, 4.30993490e-03, 6.76657118e-06, 6.59400463e-07,\n",
       "       1.04002045e-05, 1.29075725e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       5.30310156e-05, 2.38258902e-01, 3.40045309e-01, 3.30851090e-04,\n",
       "       8.83938425e-04, 6.63996587e-04, 1.04919043e-04, 3.59962693e-04,\n",
       "       4.48987327e-04, 4.29692407e-04, 4.47944169e-04, 4.70940681e-04,\n",
       "       4.66006939e-04, 3.91178056e-04, 3.94382839e-02, 1.22144786e-03,\n",
       "       2.73038461e-04, 1.47339155e-09, 4.86478579e-03, 6.06672874e-04,\n",
       "       4.67168233e-04, 4.28227253e-04, 4.58780251e-04, 1.11528318e-03,\n",
       "       3.17262399e-04, 4.02751204e-04, 2.30210307e-04, 4.20733188e-04,\n",
       "       3.30662478e-04, 3.40087564e-04, 4.15455455e-04, 6.34069353e-05,\n",
       "       7.92373409e-05, 3.20528896e-04, 3.92579236e-04, 3.98811798e-04,\n",
       "       2.16391818e-04, 4.11914111e-04, 2.81670550e-04, 3.58452236e-04,\n",
       "       4.46987626e-04, 8.24331804e-05, 1.12315628e-04, 3.41492261e-04,\n",
       "       3.45614874e-04, 1.14937132e-01, 8.94043333e-02, 1.32442118e-04,\n",
       "       8.37713169e-06, 9.41327330e-04, 1.22577228e-05, 5.38174072e-04,\n",
       "       1.78934291e-03, 4.57508002e-02, 6.62824089e-02, 6.85446286e-05])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rforestclf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost Regressor/ Adaboost Classifier with Grid SearchCV on fare , tip amount and tip paid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(data, y_fare, test_size = 0.2, \n",
    "                                                                    random_state = 2019)\n",
    "adaboost=ensemble.AdaBoostRegressor()\n",
    "parameters={'n_estimators':[61,71],\n",
    "            'learning_rate':[0.05,.1]\n",
    "           }\n",
    "adareg = model_selection.GridSearchCV(adaboost, \n",
    "                                      param_grid=parameters, \n",
    "                                      cv=model_selection.KFold(n_splits=3, shuffle=True, random_state=2019)\n",
    "                                     )\n",
    "adareg.fit(x_train,y_train.ravel())\n",
    "\n",
    "picklefile='adaboost.pkl'\n",
    "with open(picklefile, 'wb') as file:\n",
    "    pickle.dump(adareg, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7194419480654323\n"
     ]
    }
   ],
   "source": [
    "pkl_filename = 'adaboost.pkl'\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    pickle_model = pickle.load(file)\n",
    "    \n",
    "# Calculate the accuracy score and predict target values\n",
    "score = pickle_model.score(x_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance:\n",
      "\n",
      "*********************************************************************************************************************\n",
      "best parameters:  {'learning_rate': 0.05, 'n_estimators': 61}\n",
      "*********************************************************************************************************************\n",
      "best score:  0.7262429726978257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdevasish\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\mdevasish\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\mdevasish\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\mdevasish\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\mdevasish\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "print('Feature Importance:\\n')\n",
    "pd.DataFrame(adareg.cv_results_)\n",
    "print(117*'*')\n",
    "print('best parameters: ', adareg.best_params_)\n",
    "print(117*'*')\n",
    "print('best score: ',adareg.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(data, y_tipamount, test_size = 0.2, \n",
    "                                                                    random_state = 2019)\n",
    "adaboost=ensemble.AdaBoostRegressor()\n",
    "parameters={'n_estimators':[61,71],\n",
    "            'learning_rate':[0.05,.1]\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adareg = model_selection.GridSearchCV(adaboost, \n",
    "                                      param_grid=parameters, \n",
    "                                      cv=model_selection.KFold(n_splits=3, shuffle=True, random_state=2019)\n",
    "                                     )\n",
    "adareg.fit(x_train,y_train.ravel())\n",
    "\n",
    "picklefile='adaboosttipamount.pkl'\n",
    "with open(picklefile, 'wb') as file:\n",
    "    pickle.dump(adareg, file)\n",
    "\n",
    "print('Feature Importance:\\n')\n",
    "pd.DataFrame(adareg.cv_results_)\n",
    "print(117*'*')\n",
    "print('best parameters: ', adareg.best_params_)\n",
    "print(117*'*')\n",
    "print('best score: ',adareg.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-44.74536060076069\n"
     ]
    }
   ],
   "source": [
    "picklefile='adaboosttipamount.pkl'\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    pickle_model = pickle.load(file)\n",
    "    \n",
    "# Calculate the accuracy score and predict target values\n",
    "score = pickle_model.score(x_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(data, y_tip_paid, test_size = 0.2, \n",
    "                                                                    random_state = 2019)\n",
    "adaboost=ensemble.AdaBoostClassifier()\n",
    "parameters={'n_estimators':[61,71],\n",
    "            'learning_rate':[0.05,.1]\n",
    "           }\n",
    "adaclf = model_selection.GridSearchCV(adaboost, \n",
    "                                      param_grid=parameters, \n",
    "                                      cv=model_selection.StratifiedKFold(n_splits=3, shuffle=True, random_state=2019)\n",
    "                                     )\n",
    "adaclf.fit(x_train,y_train.ravel())\n",
    "\n",
    "picklefile='adaboosttippaid.pkl'\n",
    "with open(picklefile, 'wb') as file:\n",
    "    pickle.dump(adaclf, file)\n",
    "\n",
    "print('Feature Importance:\\n')\n",
    "pd.DataFrame(adaclf.cv_results_)\n",
    "print(117*'*')\n",
    "print('best parameters: ', adaclf.best_params_)\n",
    "print(117*'*')\n",
    "print('best score: ',adaclf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-44.74536060076069\n"
     ]
    }
   ],
   "source": [
    "picklefile='adaboosttippaid.pkl'\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    pickle_model = pickle.load(file)\n",
    "    \n",
    "# Calculate the accuracy score and predict target values\n",
    "score = pickle_model.score(x_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regressor/ Gradient Boosting Classifier with Grid SearchCV on fare , tip amount and tip paid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-f5307408bd11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m                                       \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                                       cv=model_selection.KFold(n_splits=3, shuffle=True, random_state=2019))\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mgboostreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mpicklefile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gboostfare.pkl'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[1;32m--> 639\u001b[1;33m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m   1027\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Presorting is not supported for sparse matrices.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m                 X_idx_sorted = np.asfortranarray(np.argsort(X, axis=0),\n\u001b[0m\u001b[0;32m   1030\u001b[0m                                                  dtype=np.int32)\n\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36margsort\u001b[1;34m(a, axis, kind, order)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m     \"\"\"\n\u001b[1;32m--> 940\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'argsort'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(data, y_fare, test_size = 0.2, \n",
    "                                                                    random_state = 2019)\n",
    "\n",
    "parameters={'n_estimators':[101,103],\n",
    "            'learning_rate':[0.05,0.1]\n",
    "           }\n",
    "\n",
    "gboost = ensemble.GradientBoostingRegressor(random_state = 2019)\n",
    "\n",
    "gboostreg = model_selection.GridSearchCV(gboost, \n",
    "                                      param_grid=parameters, \n",
    "                                      cv=model_selection.KFold(n_splits=3, shuffle=True, random_state=2019))\n",
    "gboostreg.fit(x_train, y_train.ravel())\n",
    "\n",
    "picklefile='gboostfare.pkl'\n",
    "with open(picklefile, 'wb') as file:\n",
    "    pickle.dump(gboostreg, file)\n",
    "\n",
    "print('Feature Importance:\\n')\n",
    "pd.DataFrame(gboostreg.cv_results_)\n",
    "print(117*'*')\n",
    "print('best parameters: ', gboostreg.best_params_)\n",
    "print(117*'*')\n",
    "print('best score: ',gboostreg.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7194419480654323\n"
     ]
    }
   ],
   "source": [
    "picklefile='gboostfare.pkl'\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    pickle_model = pickle.load(file)\n",
    "    \n",
    "# Calculate the accuracy score and predict target values\n",
    "score = pickle_model.score(x_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(data, y_tipamount, test_size = 0.2, \n",
    "                                                                    random_state = 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(data, y_tipamount, test_size = 0.2, \n",
    "                                                                    random_state = 2019)\n",
    "\n",
    "parameters={'n_estimators':[101,103],\n",
    "            'learning_rate':[0.05,0.1]\n",
    "           }\n",
    "\n",
    "gboost = ensemble.GradientBoostingRegressor(random_state = 2019)\n",
    "\n",
    "gboostreg = model_selection.GridSearchCV(gboost, \n",
    "                                      param_grid=parameters, \n",
    "                                      cv=model_selection.KFold(n_splits=3, shuffle=True, random_state=2019))\n",
    "gboostreg.fit(x_train, y_train.ravel())\n",
    "\n",
    "picklefile='gboosttipamount.pkl'\n",
    "with open(picklefile, 'wb') as file:\n",
    "    pickle.dump(gboostreg, file)\n",
    "\n",
    "print('Feature Importance:\\n')\n",
    "pd.DataFrame(gboostreg.cv_results_)\n",
    "print(117*'*')\n",
    "print('best parameters: ', gboostreg.best_params_)\n",
    "print(117*'*')\n",
    "print('best score: ',gboostreg.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-44.74536060076069\n"
     ]
    }
   ],
   "source": [
    "picklefile='gboosttipamount.pkl'\n",
    "\n",
    "with open(pkl_filename, 'rb') as file:\n",
    "    pickle_model = pickle.load(file)\n",
    "    \n",
    "# Calculate the accuracy score and predict target values\n",
    "score = pickle_model.score(x_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(data, y_tip_paid, test_size = 0.2, \n",
    "                                                                    random_state = 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=3, random_state=2019, shuffle=True),\n",
       "       error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=2019, subsample=1.0, verbose=0,\n",
       "              warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [101, 103], 'learning_rate': [0.05, 0.1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(data, y_tip_paid, test_size = 0.2, \n",
    "                                                                    random_state = 2019)\n",
    "\n",
    "parameters={'n_estimators':[101,103],\n",
    "            'learning_rate':[0.05,0.1]\n",
    "           }\n",
    "\n",
    "gboost = ensemble.GradientBoostingClassifier(random_state=2019)\n",
    "\n",
    "gboostclf = model_selection.GridSearchCV(gboost, \n",
    "                                      param_grid=parameters, \n",
    "                                      cv=model_selection.StratifiedKFold(n_splits=3, shuffle=True, random_state=2019))\n",
    "gboostclf.fit(x_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance:\n",
      "\n",
      "*********************************************************************************************************************\n",
      "best parameters:  {'learning_rate': 0.05, 'n_estimators': 101}\n",
      "*********************************************************************************************************************\n",
      "best score:  0.9886840496095373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdevasish\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\mdevasish\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\mdevasish\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\mdevasish\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\Users\\mdevasish\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "#picklefile='gboosttippaid.pkl'\n",
    "#with open(picklefile, 'wb') as file:\n",
    "#    pickle.dump(gboostclf, file)\n",
    "\n",
    "print('Feature Importance:\\n')\n",
    "pd.DataFrame(gboostclf.cv_results_)\n",
    "print(117*'*')\n",
    "print('best parameters: ', gboostclf.best_params_)\n",
    "print(117*'*')\n",
    "print('best score: ',gboostclf.best_score_)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Attachments",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
